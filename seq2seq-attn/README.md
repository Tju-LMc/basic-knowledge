# Seq2Seq Attention

一个非常好的warmup教程

`python main.py` 即可

取自pytorch官网[NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)

对数据集进行了删减不用再单独下载, 调整了部分参数加快加速度

注意打印出的dataset path, 建议一步步debug下forward过程, 配合教程中的图片, 理解attention机制
